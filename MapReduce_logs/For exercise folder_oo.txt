$ docker run -v $(pwd):/usr/local/hadoop/py -it sequenceiq/hadoop-docker:2.7.1 /usr/local/hadoop/py/py_runner.sh exercise "oo"
/
Starting sshd:                                             [  OK  ]
20/04/06 05:54:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [e20af112ad5e]
e20af112ad5e: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-e20af112ad5e.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-e20af112ad5e.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-e20af112ad5e.out
20/04/06 05:55:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-e20af112ad5e.out
localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-e20af112ad5e.out
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

20/04/06 05:55:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Safe mode is OFF
20/04/06 05:55:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/04/06 05:55:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/04/06 05:55:23 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
20/04/06 05:55:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
packageJobJar: [/usr/local/hadoop/py/basic_grep/mapper.py, /usr/local/hadoop/py/basic_grep/reducer.py, /tmp/hadoop-unjar2221195804787702090/] [] /tmp/streamjob1219564257098685594.jar tmpDir=null
20/04/06 05:55:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/04/06 05:55:25 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/04/06 05:55:25 INFO mapred.FileInputFormat: Total input paths to process : 2
20/04/06 05:55:26 INFO mapreduce.JobSubmitter: number of splits:2
20/04/06 05:55:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1586166912780_0001
20/04/06 05:55:26 INFO impl.YarnClientImpl: Submitted application application_1586166912780_0001
20/04/06 05:55:26 INFO mapreduce.Job: The url to track the job: http://e20af112ad5e:8088/proxy/application_1586166912780_0001/
20/04/06 05:55:26 INFO mapreduce.Job: Running job: job_1586166912780_0001
20/04/06 05:55:34 INFO mapreduce.Job: Job job_1586166912780_0001 running in uber mode : false
20/04/06 05:55:34 INFO mapreduce.Job:  map 0% reduce 0%
20/04/06 05:55:43 INFO mapreduce.Job:  map 100% reduce 0%
20/04/06 05:55:49 INFO mapreduce.Job:  map 100% reduce 100%
20/04/06 05:55:49 INFO mapreduce.Job: Job job_1586166912780_0001 completed successfully
20/04/06 05:55:49 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=124
                FILE: Number of bytes written=357670
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=276
                HDFS: Number of bytes written=6
                HDFS: Number of read operations=9
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters
                Launched map tasks=2
                Launched reduce tasks=1
                Data-local map tasks=2
                Total time spent by all maps in occupied slots (ms)=11728
                Total time spent by all reduces in occupied slots (ms)=3938
                Total time spent by all map tasks (ms)=11728
                Total time spent by all reduce tasks (ms)=3938
                Total vcore-seconds taken by all map tasks=11728
                Total vcore-seconds taken by all reduce tasks=3938
                Total megabyte-seconds taken by all map tasks=12009472
                Total megabyte-seconds taken by all reduce tasks=4032512
        Map-Reduce Framework
                Map input records=2
                Map output records=14
                Map output bytes=90
                Map output materialized bytes=130
                Input split bytes=214
                Combine input records=0
                Combine output records=0
                Reduce input groups=4
                Reduce shuffle bytes=130
                Reduce input records=14
                Reduce output records=1
                Spilled Records=28
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=119
                CPU time spent (ms)=2330
                Physical memory (bytes) snapshot=724746240
                Virtual memory (bytes) snapshot=2198818816
                Total committed heap usage (bytes)=524812288
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=62
        File Output Format Counters
                Bytes Written=6
20/04/06 05:55:49 INFO streaming.StreamJob: Output directory: /usr/local/hadoop/py-output
20/04/06 05:55:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
foo     6
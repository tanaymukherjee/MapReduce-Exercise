$ docker run -v $(pwd):/usr/local/hadoop/py -it sequenceiq/hadoop-docker:2.7.1 /usr/local/hadoop/py/py_runner.sh exercise "^k[a-zA-Z]*e$"
/
Starting sshd:                                             [  OK  ]
20/04/06 06:18:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [2e05f96cb991]
2e05f96cb991: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-2e05f96cb991.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-2e05f96cb991.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-2e05f96cb991.out
20/04/06 06:18:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-2e05f96cb991.out
localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-2e05f96cb991.out
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

20/04/06 06:18:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Safe mode is OFF
20/04/06 06:18:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/04/06 06:18:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/04/06 06:18:59 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
20/04/06 06:18:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
packageJobJar: [/usr/local/hadoop/py/basic_grep/mapper.py, /usr/local/hadoop/py/basic_grep/reducer.py, /tmp/hadoop-unjar8061670284258871836/] [] /tmp/streamjob6096975942505952419.jar tmpDir=null
20/04/06 06:19:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/04/06 06:19:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/04/06 06:19:02 INFO mapred.FileInputFormat: Total input paths to process : 2
20/04/06 06:19:02 INFO mapreduce.JobSubmitter: number of splits:2
20/04/06 06:19:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1586168326792_0001
20/04/06 06:19:03 INFO impl.YarnClientImpl: Submitted application application_1586168326792_0001
20/04/06 06:19:03 INFO mapreduce.Job: The url to track the job: http://2e05f96cb991:8088/proxy/application_1586168326792_0001/
20/04/06 06:19:03 INFO mapreduce.Job: Running job: job_1586168326792_0001
20/04/06 06:19:11 INFO mapreduce.Job: Job job_1586168326792_0001 running in uber mode : false
20/04/06 06:19:11 INFO mapreduce.Job:  map 0% reduce 0%
20/04/06 06:19:20 INFO mapreduce.Job:  map 100% reduce 0%
20/04/06 06:19:26 INFO mapreduce.Job:  map 100% reduce 100%
20/04/06 06:19:27 INFO mapreduce.Job: Job job_1586168326792_0001 completed successfully
20/04/06 06:19:27 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=244
                FILE: Number of bytes written=358024
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=349
                HDFS: Number of bytes written=16
                HDFS: Number of read operations=9
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters
                Launched map tasks=2
                Launched reduce tasks=1
                Data-local map tasks=2
                Total time spent by all maps in occupied slots (ms)=13110
                Total time spent by all reduces in occupied slots (ms)=3939
                Total time spent by all map tasks (ms)=13110
                Total time spent by all reduce tasks (ms)=3939
                Total vcore-seconds taken by all map tasks=13110
                Total vcore-seconds taken by all reduce tasks=3939
                Total megabyte-seconds taken by all map tasks=13424640
                Total megabyte-seconds taken by all reduce tasks=4033536
        Map-Reduce Framework
                Map input records=2
                Map output records=26
                Map output bytes=186
                Map output materialized bytes=250
                Input split bytes=214
                Combine input records=0
                Combine output records=0
                Reduce input groups=16
                Reduce shuffle bytes=250
                Reduce input records=26
                Reduce output records=2
                Spilled Records=52
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=97
                CPU time spent (ms)=2120
                Physical memory (bytes) snapshot=687792128
                Virtual memory (bytes) snapshot=2200408064
                Total committed heap usage (bytes)=516423680
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=135
        File Output Format Counters
                Bytes Written=16
20/04/06 06:19:27 INFO streaming.StreamJob: Output directory: /usr/local/hadoop/py-output
20/04/06 06:19:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
kettle  1
kite    1